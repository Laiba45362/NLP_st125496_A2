{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQEiJV_H_aaR",
        "outputId": "ca034980-6ae2-482a-a628-b636a8edb1db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sequences: 48245\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download the 'punkt_tab' resource:\n",
        "nltk.download('punkt_tab') # This line has been added to download 'punkt_tab'\n",
        "\n",
        "# Load the dataset (example for Project Gutenberg text)\n",
        "# Change the URL to \"The Adventures of Sherlock Holmes\"\n",
        "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "\n",
        "# Save the data to a file\n",
        "with open(\"dataset.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(data)\n",
        "\n",
        "# Load the dataset from file\n",
        "with open('dataset.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenization\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Lowercasing\n",
        "tokens = [token.lower() for token in tokens]\n",
        "\n",
        "# Removing punctuation and special characters\n",
        "tokens = [re.sub(r'\\W+', '', token) for token in tokens if re.sub(r'\\W+', '', token)]\n",
        "\n",
        "# Removing stop words (optional)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Add a special token for unknown words\n",
        "tokens.append('')\n",
        "\n",
        "# Numericalization\n",
        "vocab = list(set(tokens))\n",
        "word2index = {word: i for i, word in enumerate(vocab)}\n",
        "index2word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "# Creating sequences\n",
        "sequence_length = 5\n",
        "sequences = []\n",
        "for i in range(len(tokens) - sequence_length):\n",
        "    sequences.append(tokens[i:i + sequence_length])\n",
        "\n",
        "# Convert sequences to numerical indices\n",
        "input_sequences = []\n",
        "for sequence in sequences:\n",
        "    input_sequences.append([word2index[word] for word in sequence])\n",
        "\n",
        "# Convert to numpy array\n",
        "input_sequences = np.array(input_sequences)\n",
        "\n",
        "print(f\"Total sequences: {len(input_sequences)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        x = self.embedding(x)\n",
        "        x, state = self.lstm(x, prev_state)\n",
        "        x = self.fc(x)\n",
        "        return x, state\n",
        "\n",
        "    def init_state(self, batch_size=1):\n",
        "        return (torch.zeros(2, batch_size, self.lstm.hidden_size),\n",
        "                torch.zeros(2, batch_size, self.lstm.hidden_size))"
      ],
      "metadata": {
        "id": "fgotk8KfFhBC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fgngy_CNGPzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 50\n",
        "hidden_dim = 100\n",
        "vocab_size = len(vocab)\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Model, loss function, optimizer\n",
        "model = LanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, input_sequences, criterion, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(0, len(input_sequences) - batch_size, batch_size):\n",
        "            inputs = torch.tensor(input_sequences[i:i + batch_size, :-1], dtype=torch.long)\n",
        "            targets = torch.tensor(input_sequences[i:i + batch_size, 1:], dtype=torch.long)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            state_h, state_c = model.init_state(batch_size)\n",
        "            outputs, _ = model(inputs, (state_h, state_c))\n",
        "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / (len(input_sequences) // batch_size)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, input_sequences, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byfhIjozFlJL",
        "outputId": "73c47787-5710-4ef3-cb35-1a20464fda0f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 8.2555\n",
            "Epoch 2/10, Loss: 7.5854\n",
            "Epoch 3/10, Loss: 7.3605\n",
            "Epoch 4/10, Loss: 7.1419\n",
            "Epoch 5/10, Loss: 6.8724\n",
            "Epoch 6/10, Loss: 6.5771\n",
            "Epoch 7/10, Loss: 6.2849\n",
            "Epoch 8/10, Loss: 6.0021\n",
            "Epoch 9/10, Loss: 5.7281\n",
            "Epoch 10/10, Loss: 5.4639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model's state dictionary\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPXzoM1OFpuC",
        "outputId": "368e8450-d0d3-4a6f-c5b4-c22bb0216ccf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_text, max_length=50):\n",
        "    model.eval()\n",
        "    words = start_text.split()\n",
        "    state_h, state_c = model.init_state(batch_size=1)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Use the word2index dictionary with a fallback to  if the word is not found\n",
        "        x = torch.tensor([[word2index.get(w, word2index['']) for w in words]], dtype=torch.long)\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(index2word[word_index])\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Test the model with a sample input\n",
        "start_text = \"\"\n",
        "generated_text = generate_text(model, start_text)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVB2uiSvInL1",
        "outputId": "1f93af49-bfa0-4c98-a315-0345808f5e12"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "harry potter is unbuttoned hunt efforts dwell shall shoulders imitated door coachman system whenever united tone empty forbidden nature swore towards jowl manners mean paced fads nicely lithe love violet violet copyright physical pounds league warranties invalidity punitive striving agreement wwwgutenbergorglicense requirements project gutenberg marry bottom unprotected horrible lieu next pass ebook four\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDvFfG4jIqUM",
        "outputId": "b1e84b0e-ec77-494f-ee2c-ebbb1a2e6373"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.23.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.41.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "# Define the model class (this should match your trained model's class)\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        x = self.embedding(x)\n",
        "        x, state = self.lstm(x, prev_state)\n",
        "        x = self.fc(x)\n",
        "        return x, state\n",
        "\n",
        "    def init_state(self, batch_size=1):\n",
        "        return (torch.zeros(2, batch_size, self.lstm.hidden_size),\n",
        "                torch.zeros(2, batch_size, self.lstm.hidden_size))\n",
        "\n",
        "# Load the trained model\n",
        "model = LanguageModel(vocab_size=len(vocab), embedding_dim=50, hidden_dim=100)\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Define the text generation function\n",
        "def generate_text(model, start_text, max_length=50):\n",
        "    model.eval()\n",
        "    words = start_text.split()\n",
        "    state_h, state_c = model.init_state(batch_size=1)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Use the word2index dictionary with a fallback to  if the word is not found\n",
        "        x = torch.tensor([[word2index.get(w, word2index['']) for w in words]], dtype=torch.long)\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(index2word[word_index])\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Streamlit app interface\n",
        "st.title(\"Text Generation with Language Model\")\n",
        "st.write(\"Enter a text prompt and the model will generate a continuation of the text.\")\n",
        "\n",
        "# Input box for user to type in a text prompt\n",
        "user_input = st.text_input(\"Enter text prompt:\", \"Sherlock Holmes solves the\")\n",
        "\n",
        "# Generate text based on user input\n",
        "if user_input:\n",
        "    with st.spinner('Generating text...'):\n",
        "        generated_text = generate_text(model, user_input)\n",
        "        st.success(\"Generated Text:\")\n",
        "        st.write(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtrGLuMEIvAS",
        "outputId": "371855bd-3ac3-413b-b33c-4381d87dd448"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-5d35f553ab4c>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('model.pth'))\n",
            "2025-01-26 15:05:12.004 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.005 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.007 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.009 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.010 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.012 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.013 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.014 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.017 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.018 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.019 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.020 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.021 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.022 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.024 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.139 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.141 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.144 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.147 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.148 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.150 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 15:05:12.152 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    }
  ]
}